# ðŸ“Š Structured Logging Guide

## ðŸŽ¯ **Overview**

The Job Search Application uses enterprise-grade structured logging for better debugging, monitoring, and production operations. All print() statements have been replaced with proper logging.

## ðŸ—ï¸ **Logging Architecture**

```
Logging System Architecture:
â”œâ”€â”€ ðŸ“ src/job_search/core/logging_config.py  # Central logging configuration
â”œâ”€â”€ ðŸ“Š logs/                                  # Log files directory (auto-created)
â”‚   â”œâ”€â”€ job_search.log                        # Main application logs
â”‚   â”œâ”€â”€ errors.log                            # Error-level logs only
â”‚   â””â”€â”€ scraping.log                          # Background task logs
â””â”€â”€ ðŸŽ¨ Console Output                         # Colored, structured output
```

## âœ¨ **Key Features**

### **ðŸŽ¨ Colored Console Output**
- **Green** INFO messages for normal operations 
- **Yellow** WARNING messages for potential issues
- **Red** ERROR messages for failures
- **Cyan** DEBUG messages for detailed debugging
- **Magenta** CRITICAL messages for severe issues

### **ðŸ“ File-Based Logging**
- **Automatic Rotation**: 10MB files, 5 backups per log type
- **Separate Files**: Main, errors, and scraping logs isolated
- **Structured Format**: Timestamp, level, module, function, message

### **ðŸ·ï¸ Enhanced Context**
- **Module Information**: Shows which component generated the log
- **Function Names**: Exact function generating the message
- **Timestamps**: Precise timing for debugging sequences
- **Emojis**: Visual indicators for quick scanning

## ðŸ”§ **Configuration**

### **Environment Variables**
```bash
# Log level (default: INFO)
LOG_LEVEL=DEBUG|INFO|WARNING|ERROR|CRITICAL

# Enable/disable file logging (default: true)
LOG_TO_FILE=true|false

# Log directory path (default: logs)
LOG_DIR=logs

# Environment-specific settings
CELERY_ENVIRONMENT=development|production|testing
```

### **Per-Service Configuration**
Each service can have independent log levels:
```bash
# Backend API detailed debugging
LOG_LEVEL=DEBUG docker-compose up backend

# Worker minimal logging
LOG_LEVEL=WARNING docker-compose up worker

# Production monitoring
LOG_LEVEL=INFO ENABLE_CELERY_MONITORING=true docker-compose up
```

## ðŸ“ **Log Formats**

### **Console Format (Colored)**
```
[14:20:28] INFO | [core.logging]        | ðŸš€ Job Search API starting up
[14:20:28] INFO | [ml.embeddings]       | âœ… HuggingFace model loaded successfully
[14:20:29] ERROR| [scraping.tasks]      | âŒ Failed to scrape Remote OK: timeout
```

### **File Format (Detailed)**
```
[2025-07-30 14:20:28] INFO | job_search.core.logging | setup_logging | Logging initialized - Level: INFO, File logging: True
[2025-07-30 14:20:28] INFO | job_search.main         | create_app    | Creating FastAPI application in cloud-ml mode
[2025-07-30 14:20:29] ERROR| job_search.scraping.tasks | crawl_and_index | Failed to scrape Remote OK: Connection timeout
```

## ðŸ› ï¸ **Usage Examples**

### **Basic Logging in Code**
```python
from ..core.logging_config import get_logger

logger = get_logger(__name__)

# Different log levels
logger.debug("ðŸ” Detailed debugging information")
logger.info("âœ… Normal operation completed")
logger.warning("âš ï¸ Potential issue detected")
logger.error("âŒ Error occurred during processing")
logger.critical("ðŸš¨ Critical system failure")
```

### **Logging with Context**
```python
# Log with variables and context
logger.info(f"ðŸ“° Processing {len(jobs)} jobs from {source}")
logger.error(f"âŒ Database connection failed: {error_msg}")

# Log function entry/exit
logger.debug("ðŸ”„ Starting job processing pipeline")
# ... processing logic ...
logger.debug("âœ… Job processing pipeline completed")
```

### **Exception Logging**
```python
try:
    # risky operation
    result = process_jobs()
    logger.info(f"âœ… Successfully processed {len(result)} jobs")
except Exception as e:
    logger.error(f"âŒ Job processing failed: {e}")
    logger.debug("ðŸ“‹ Full traceback:", exc_info=True)
    raise
```

## ðŸ“Š **Monitoring & Analysis**

### **Real-Time Log Monitoring**
```bash
# Watch all logs
tail -f logs/job_search.log

# Watch only errors
tail -f logs/errors.log

# Watch background tasks
tail -f logs/scraping.log

# Filter by log level
grep "ERROR" logs/job_search.log

# Search for specific component
grep "ml.embeddings" logs/job_search.log
```

### **Docker Service Logs**
```bash
# All services combined
docker-compose -f docker/docker-compose-fullstack.yml logs -f

# Specific service
docker-compose -f docker/docker-compose-fullstack.yml logs -f backend
docker-compose -f docker/docker-compose-fullstack.yml logs -f worker
docker-compose -f docker/docker-compose-fullstack.yml logs -f scheduler

# Last N lines
docker-compose -f docker/docker-compose-fullstack.yml logs --tail=100 backend
```

### **Log Analysis Examples**
```bash
# Count error types
grep "ERROR" logs/job_search.log | cut -d'|' -f3 | sort | uniq -c

# Find slow operations (if timing included)
grep "completed in" logs/job_search.log | sort -k4 -n

# Monitor API requests
grep "POST\|GET\|PUT\|DELETE" logs/job_search.log

# Track background job execution
grep "ðŸ”„\|âœ…\|âŒ" logs/scraping.log
```

## ðŸš¨ **Error Handling & Alerting**

### **Error Severity Levels**
- **WARNING**: Recoverable issues, system continues operating
- **ERROR**: Failed operations, but service remains available  
- **CRITICAL**: Severe failures requiring immediate attention

### **Common Error Patterns**
```python
# Network/API errors
logger.error(f"âŒ Failed to connect to {service}: {error}")

# Data processing errors  
logger.error(f"ðŸ’¥ Failed to process job {job_id}: {error}")

# Configuration errors
logger.critical(f"ðŸš¨ Missing required config: {config_key}")

# Resource errors
logger.warning(f"âš ï¸ High memory usage: {memory_percent}%")
```

### **Integration with Monitoring Tools**
The structured logs can be easily integrated with:
- **ELK Stack** (Elasticsearch, Logstash, Kibana)
- **Prometheus + Grafana** for metrics
- **Sentry** for error tracking
- **DataDog** for comprehensive monitoring

## ðŸ”§ **Troubleshooting**

### **Log File Issues**
```bash
# Check log directory permissions
ls -la logs/

# Verify log rotation is working
ls -lh logs/job_search.log*

# Check disk space
df -h logs/
```

### **Missing Logs**
```bash
# Verify LOG_TO_FILE setting
echo $LOG_TO_FILE

# Check log level configuration
echo $LOG_LEVEL

# Manually test logging
python -c "from src.job_search.core.logging_config import setup_logging, get_logger; setup_logging(); logger = get_logger('test'); logger.info('Test message')"
```

### **Performance Impact**
- **File I/O**: Minimal impact with async logging
- **Console Output**: Slight overhead in development mode
- **Production**: Use INFO level or higher for optimal performance

## ðŸŽ¯ **Best Practices**

### **Do's**
âœ… Use appropriate log levels (DEBUG for debugging, INFO for flow, ERROR for failures)  
âœ… Include context and variables in log messages  
âœ… Use emojis for visual scanning and categorization  
âœ… Log function entry/exit for complex operations  
âœ… Include error details and potential causes  

### **Don'ts**
âŒ Don't log sensitive information (passwords, API keys, personal data)  
âŒ Don't use DEBUG level in production (performance impact)  
âŒ Don't log in tight loops without rate limiting  
âŒ Don't ignore log rotation (can fill disk space)  
âŒ Don't use generic error messages without context  

### **Log Message Guidelines**
```python
# Good: Specific, contextual, actionable
logger.error(f"âŒ Failed to save job {job_id} for user {user_id}: Database connection timeout")

# Bad: Generic, no context
logger.error("Error occurred")

# Good: Clear operation flow
logger.info(f"ðŸ”„ Starting ML search for query: '{query}' (max_results: {max_results})")

# Bad: Unclear purpose
logger.info("Processing request")
```

## ðŸ“ˆ **Performance Monitoring**

### **Key Metrics to Track**
- **Log Volume**: Messages per minute/hour
- **Error Rate**: ERROR/WARNING ratio to total logs
- **Response Times**: If included in log messages
- **Resource Usage**: Memory/CPU impact of logging

### **Log-Based Alerts**
Set up alerts for:
- High error rates (>5% ERROR messages)
- Critical errors (any CRITICAL level)
- Service unavailability patterns
- Background job failures

---

**ðŸ’¡ Tip**: Use the monitoring script for comprehensive health checks:
```bash
python scripts/monitor_services.py --continuous
```